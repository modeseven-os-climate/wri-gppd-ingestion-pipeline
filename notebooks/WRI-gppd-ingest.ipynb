{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41652f59-1798-4431-90dc-592dd4f64a7f",
   "metadata": {},
   "source": [
    "## Load WRI Power Plant data from 2019 dataset (see https://datasets.wri.org/dataset/globalpowerplantdatabase) for original source\n",
    "\n",
    "Copyright (C) 2021 OS-Climate\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "### We have a local copy rooted in the S3_BUCKET : WRI/global_power_plant_database_v_1_3/global_power_plant_database.csv\n",
    "### To tidy the data we factor into three tables:\n",
    "\n",
    "* **wri_plants** (all the fixed data about each plant)\n",
    "* **wri_annual_gwh** (per plant/per year annual generation in GWh)\n",
    "* **wri_estimated_gwh** (per plant/per year estimated generation in GWh)\n",
    "\n",
    "### The next step is to enrich with OS-C Factor metadata\n",
    "\n",
    "Contributed by Michael Tiemann (Github: MichaelTiemannOSC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92377eb7-1d1b-4662-ac08-99877153832b",
   "metadata": {},
   "source": [
    "Load Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac18bf3b-80d7-4b25-8ae4-9273709a0789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the AWS Account page, copy the export scripts from the appropriate role using the \"Command Line or Programmatic Access\" link\n",
    "# Paste the copied text into ~/credentials.env\n",
    "\n",
    "from dotenv import dotenv_values, load_dotenv\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "dotenv_dir = os.environ.get('CREDENTIAL_DOTENV_DIR', os.environ.get('PWD', '/opt/app-root/src'))\n",
    "dotenv_path = pathlib.Path(dotenv_dir) / 'credentials.env'\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path=dotenv_path,override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5877739-a3c7-4f6c-ae92-b2288f511b41",
   "metadata": {},
   "source": [
    "Create an S3 resource for the bucket holding source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3207a36b-b6ec-4694-841b-160a13a8a1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3_resource = boto3.resource(\n",
    "    service_name=\"s3\",\n",
    "    endpoint_url=os.environ['S3_LANDING_ENDPOINT'],\n",
    "    aws_access_key_id=os.environ['S3_LANDING_ACCESS_KEY'],\n",
    "    aws_secret_access_key=os.environ['S3_LANDING_SECRET_KEY'],\n",
    ")\n",
    "bucket = s3_resource.Bucket(os.environ['S3_LANDING_BUCKET'])\n",
    "\n",
    "wri_file = bucket.Object('WRI/global_power_plant_database_v_1_3/global_power_plant_database.csv').get()['Body']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08559f9f-e588-40f6-b842-09908b260183",
   "metadata": {},
   "source": [
    "Load WRI data file using pandas *read_csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0800f2-5c35-4f06-90da-b8b00d597ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Because NaN cannot be converted to int type, we cannot use int as a datatype for years that are NaN\n",
    "\n",
    "wri_df = pd.read_csv(wri_file, dtype={'latitude':'float64', 'longitude':'float64', 'capacity_mw':'float64', 'other_fuel3':'str'})\n",
    "\n",
    "display(wri_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415edebf-754a-4f1b-848a-d035651e810c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Melt the generation data into a more tidy format, dropping NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e13313d-1cd4-4650-bfe4-1918c9f41f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "wri_plants = wri_df[['country', 'country_long', 'name', 'gppd_idnr', 'capacity_mw',\n",
    "       'latitude', 'longitude', 'primary_fuel', 'other_fuel1', 'other_fuel2',\n",
    "       'other_fuel3', 'commissioning_year', 'owner', 'source', 'url',\n",
    "       'geolocation_source', 'wepp_id', 'year_of_capacity_data', 'generation_data_source']]\n",
    "\n",
    "wri_id_vars = ['gppd_idnr']\n",
    "wri_value_vars = ['generation_gwh_2013', 'generation_gwh_2014', 'generation_gwh_2015', 'generation_gwh_2016',\n",
    "                  'generation_gwh_2017', 'generation_gwh_2018', 'generation_gwh_2019']\n",
    "wri_annual_gwh = wri_df.melt(wri_id_vars, wri_value_vars, var_name='year', value_name='generation_gwh')\n",
    "wri_annual_gwh['year'] = pd.to_numeric(wri_annual_gwh['year'].apply(lambda x: int(x.split('_')[-1])))\n",
    "wri_annual_gwh.dropna(subset=['generation_gwh'],inplace=True)\n",
    "\n",
    "display(wri_annual_gwh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75af6089-c252-4ca9-9915-9c2613e77496",
   "metadata": {},
   "outputs": [],
   "source": [
    "wri_id_vars = ['gppd_idnr']\n",
    "wri_value_vars = ['estimated_generation_gwh_2013', 'estimated_generation_gwh_2014', 'estimated_generation_gwh_2015',\n",
    "                  'estimated_generation_gwh_2016', 'estimated_generation_gwh_2017']\n",
    "wri_estimated_gwh = wri_df.melt(wri_id_vars, wri_value_vars, var_name='year', value_name='estimated_generation_gwh')\n",
    "wri_estimated_gwh['year'] = pd.to_numeric(wri_estimated_gwh['year'].apply(lambda x: int(x.split('_')[-1])))\n",
    "\n",
    "wri_value_vars = ['estimated_generation_note_2013', 'estimated_generation_note_2014', 'estimated_generation_note_2015',\n",
    "                  'estimated_generation_note_2016', 'estimated_generation_note_2017']\n",
    "wri_estimated_note = wri_df.melt(wri_id_vars, wri_value_vars, var_name='year', value_name='estimated_generation_note')\n",
    "wri_estimated_note['year'] = pd.to_numeric(wri_estimated_note['year'].apply(lambda x: int(x.split('_')[-1])))\n",
    "\n",
    "display(wri_estimated_gwh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ce63fc-7e82-45ec-8af4-44dff1ae1e43",
   "metadata": {},
   "source": [
    "Merge the estimation data so that estimates and notes are 1:1 together, dropping NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042ff422-b471-4979-a560-5074dc5d7656",
   "metadata": {},
   "outputs": [],
   "source": [
    "wri_estimated_gwh = wri_estimated_gwh.merge(wri_estimated_note, on=['gppd_idnr', 'year'], validate=\"one_to_one\")\n",
    "\n",
    "wri_estimated_gwh.dropna(subset=['estimated_generation_gwh'],inplace=True)\n",
    "\n",
    "display(wri_estimated_gwh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b39786-15d8-430f-9111-b05fc08e73f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(wri_plants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e451728-d811-4ce3-9c11-bf03e5ce0feb",
   "metadata": {},
   "source": [
    "Ingest the data onto the S3 Development Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b3d6dc-692c-4bc9-97d5-432672b739c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaname = 'wri_gppd'\n",
    "tablename_to_df = {'plants': wri_plants, 'annual_gwh': wri_annual_gwh, 'estimated_gwh': wri_estimated_gwh}\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client(\n",
    "    service_name=\"s3\",\n",
    "    endpoint_url=os.environ['S3_DEV_ENDPOINT'],\n",
    "    aws_access_key_id=os.environ['S3_DEV_ACCESS_KEY'],\n",
    "    aws_secret_access_key=os.environ['S3_DEV_SECRET_KEY'],\n",
    ")\n",
    "\n",
    "for tablename, df in tablename_to_df.items():\n",
    "    df.to_parquet('/tmp/{sname}.{tname}.parquet'.format(sname=schemaname, tname=tablename), index=False)\n",
    "\n",
    "    s3.upload_file(\n",
    "        Bucket=os.environ['S3_DEV_BUCKET'],\n",
    "        Key='trino/{sname}/{tname}/{tname}.parquet'.format(sname=schemaname, tname=tablename),\n",
    "        Filename='/tmp/{sname}.{tname}.parquet'.format(sname=schemaname, tname=tablename)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce60fc0-e465-49dd-ad83-0c82aeb6e5da",
   "metadata": {},
   "source": [
    "Build a map and define schema mapping logic for parquet to sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2478a68-55fd-4f0e-8b13-38cef50687c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_p2smap = {\n",
    "    'object': 'varchar',\n",
    "    'string': 'varchar',\n",
    "    'str': 'varchar',\n",
    "    'float32': 'real',\n",
    "    'Float32': 'real',\n",
    "    'float64': 'double',\n",
    "    'Float64': 'double',\n",
    "    'int32': 'integer',\n",
    "    'Int32': 'integer',\n",
    "    'int64': 'bigint',\n",
    "    'Int64': 'bigint',\n",
    "    'category': 'varchar',\n",
    "    'datetime64[ns, UTC]': 'timestamp',\n",
    "    'datetime64[ns]': 'timestamp'\n",
    "}\n",
    "\n",
    "def pandas_type_to_sql(pt):\n",
    "    st = _p2smap.get(pt)\n",
    "    if st is not None:\n",
    "        return st\n",
    "    raise ValueError(\"unexpected pandas column type '{pt}'\".format(pt=pt))\n",
    "\n",
    "# add ability to specify optional dict for specific fields?\n",
    "# if column name is present, use specified value?\n",
    "def generate_table_schema_pairs(df):\n",
    "    ptypes = [str(e) for e in df.dtypes.to_list()]\n",
    "    stypes = [pandas_type_to_sql(e) for e in ptypes]\n",
    "    pz = list(zip(df.columns.to_list(), stypes))\n",
    "    return \",\\n\".join([\"    {n} {t}\".format(n=e[0],t=e[1]) for e in pz])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bccd8a0-3ee0-46d5-9bca-0138ab277cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trino\n",
    "\n",
    "conn = trino.dbapi.connect(\n",
    "    host=os.environ['TRINO_HOST'],\n",
    "    port=int(os.environ['TRINO_PORT']),\n",
    "    user=os.environ['TRINO_USER'],\n",
    "    http_scheme='https',\n",
    "    auth=trino.auth.JWTAuthentication(os.environ['TRINO_PASSWD']),\n",
    "    verify=True,\n",
    ")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e914817-28e0-4c95-a135-4fddf4948a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show available schemas to ensure trino connection is set correctly\n",
    "cur.execute('show schemas in osc_datacommons_dev')\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97ca610-38ae-4fdf-b7c6-a67760cc0374",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('create schema if not exists osc_datacommons_dev.' + schemaname)\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc4112b-eec8-42b7-a6e0-a7a56b6105a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('.'.join(['drop table if exists osc_datacommons_dev', schemaname, tablename]))\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146adf77-7906-4a41-ad6c-9a3e1b15569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tablename, df in tablename_to_df.items():\n",
    "    schema = generate_table_schema_pairs(df)\n",
    "\n",
    "    tabledef = \"\"\"create table if not exists osc_datacommons_dev.{sname}.{tname}(\n",
    "{schema}\n",
    ") with (\n",
    "    format = 'parquet',\n",
    "    external_location = 's3a://{bucket}/trino/{sname}/{tname}/'\n",
    ")\"\"\".format(schema=schema,bucket=os.environ['S3_DEV_BUCKET'],sname=schemaname,tname=tablename)\n",
    "    print(tabledef)\n",
    "    \n",
    "    # tables created externally may not show up immediately in cloud-beaver\n",
    "    cur.execute(tabledef)\n",
    "    cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ce11e2-9e8e-4ac9-b32b-5f694becf46d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
